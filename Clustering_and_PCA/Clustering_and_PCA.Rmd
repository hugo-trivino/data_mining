---
title: "Clustering and Principal Component Analysis"
author: "Hugo Trivino"
output: 
  html_notebook:
    toc: yes
    toc_float: yes
---

```{r}
setwd(".")
set.seed(1122)
options("digits"=3)
library(factoextra)
library(ggplot2)
library(dplyr)
library(cluster)

```

#2.1 Problem 1: K-means clustering

##(a) Data cleanup
###(i) So far I think all the attribute are relevant for the clustering.

```{r}
text2csv <- function(in.file,out.file) {
  mammals.df <- read.csv(file= in.file,
                           header = FALSE,
                           sep = "",
                           dec = ".",
                           comment.char = "#"
                           
                           )
  
  write.csv(mammals.df, file = out.file)
  mammals.df <- read.csv(file= out.file,
                           header = TRUE,
                           sep = ",",
                           dec = ".",
                           comment.char = "#",
                         skip = 4
                           )
  write.csv(mammals.df[,2:10], file = out.file,row.names = F)
  mammals.df <- read.csv(out.file,row.names = 1)
  
  return( mammals.df)
}
mammals.df <- text2csv(in.file = "file19.txt", out.file = "mammals.csv")
summary(mammals.df)
mammals.scaled <- scale(mammals.df)
rm(mammals.df)
```


###(ii) The data is not standardize since the mean is not zero.
###(iii) Data is cleaned in the function text2csv and works for all Hartigan file format, the resulting file is named "mammals.csv".

##(b) Clustering (2 points divided evenly by components below)


###(i) According to WSS, k = 7 and k=9 are good numbers, but Silhouette indicates that the best cluster number is k=8. However, by analyzing output in part (vi) I realized that k=7 was the best cluster for this application.
```{r}
fviz_nbclust(mammals.scaled, kmeans, method="wss") # Elbow method minimizes total
fviz_nbclust(mammals.scaled, kmeans, method="silhouette") # Silhouette method
```


###(ii) Plotting the clusters using fviz_cluster().

```{r}
set.seed(1122)
kmu <- kmeans(mammals.scaled, centers=7,nstart=40)
#png(filename="mammalCluster.png", units = "px", width=2000, height=2000)
fviz_cluster(kmu, data=mammals.scaled)
#dev.off()

```

###(iii) Table of observations per cluster
```{r}
obsxclus <-as.data.frame(table(kmu$cluster))
names(obsxclus) <-c("Cluster","Observations")
print(obsxclus)
rm(obsxclus)
```

###(iv) The total sum of square errors 
```{r}
print(paste("Total SSE :", kmu$tot.withinss))
```


###(v) SSE for each cluster is shown in the following table:
```{r}

SSExclus <-c()
for (x in 1:max(kmu$cluster)){
  SSExclus <-rbind(SSExclus,c(x,kmu$withinss[x]))
}
colnames(SSExclus) <- c("Cluster", "SSE")
print(SSExclus)
rm(SSExclus,x)
```

###(vi) I started with k=7, then I did k=8, and k=9. I found k=7 to be the best fit according to my limited knowledge of the animal kingdom since the others separate the squirrels in different clusters or the common mole from other mole types. This is my final group separation:

```{r}
for (x in 1:max(kmu$cluster)){
    cat(paste("Group ",x, ":\n",paste(names(which(kmu$cluster==x)),collapse = ", ")) ,"\n") 
}
rm(x)
```

#2.2 Problem 2: Hierarchical clustering

```{r}
set.seed(1122)
mammals.sample <-sample_n(read.csv("mammals.csv",row.names = 1),size = 35) 
```

##(a)Running hierarchical clustering on the data-set using factoextra::eclust() method and using the clustering algorithm for three linkages: single, complete, and average.
```{r}
hclust.single <- eclust(mammals.sample, FUNcluster = "hclust",hc_method="single")
hclust.complete <- eclust(mammals.sample, FUNcluster = "hclust",hc_method="complete")
hclust.average <- eclust(mammals.sample, FUNcluster = "hclust",hc_method="average")
```

```{r}
fviz_dend(hclust.average,show_labels=T ,palette="jco",cex = .6, hang=-.5,horiz = T,main = "Cluster Dendogram Using Average Linkage")
fviz_dend(hclust.complete,show_labels=T ,palette="jco",cex = .6, hang=-.5,horiz = T, main = "Cluster Dendogram Using Complete Linkage")
fviz_dend(hclust.single,show_labels=T ,palette="jco",cex = .6, hang=-.5,horiz = T,main = "Cluster Dendogram Using Single Linkage")

```
##(b) Finding the clusters formed by two singleton clusters.
```{r}
print2sing <- function(hc) {
  for (x in 1:34){
    if(hc$merge[x,1] <0 & hc$merge[x,2] <0){
      cat(paste("\t{",rownames(mammals.sample[ - hc$merge[x,1],] ),
                  ", ",rownames(mammals.sample[ - hc$merge[x,2],] ),"}\n"))
    }
  }
}
cat("Two singleton clusters formed using Single Linkage:\n")
print2sing(hclust.single)

cat("\nTwo singleton clusters formed using Complete Linkage\n")
print2sing(hclust.complete)

cat("\nTwo singleton clusters formed using Average Linkage\n")
print2sing(hclust.average)

```
##(c) By defining purity as the linkage strategy that produces the least two-singleton clusters, the purest linkage strategies is single linkage given that produced only 8 two singleton clusters compared to the others linkages that produced 10. I will pick Single linkage strategy for this example.

##(d) Cutting the linkage method chosen in (c) at height 2 I have 5 clusters as shown.
```{r}
plot(hclust.single,hang = -1,main = "Cutting Single Linkage Dendogram at height 2")
groups <- cutree(hclust.single, h=2) 
rect.hclust(hclust.single, h=2, border="red") 

```

##(e) Using the value k=5 (because of (d)) to create clusters using the three different linkage methods.

```{r}
hclust.single <- eclust(mammals.sample, FUNcluster = "hclust",hc_method="single",k=5)
hclust.complete <- eclust(mammals.sample, FUNcluster = "hclust",hc_method="complete",k=5)
hclust.average <- eclust(mammals.sample, FUNcluster = "hclust",hc_method="average",k=5)
fviz_dend(hclust.average,show_labels=T ,palette="jco",cex = .6, hang=-.5,horiz = T,main = "Cluster Dendogram Using Average Linkage")
fviz_dend(hclust.complete,show_labels=T ,palette="jco",cex = .6, hang=-.5,horiz = T, main = "Cluster Dendogram Using Complete Linkage")
fviz_dend(hclust.single,show_labels=T ,palette="jco",cex = .6, hang=-.5,horiz = T,main = "Cluster Dendogram Using Single Linkage")
```

##(f)Printing the Dunn and Silhouette width using fpc::cluster.stats() for each linkage method. 

```{r}
pDunnSil <-function(clust){
  distance <-dist(scale(mammals.sample))
  tt<-fpc::cluster.stats(distance,clust$cluster)
  cat(paste("Dunn :",tt$dunn," ; Silhouette :",tt$avg.silwidth ,"\n"))
}

print("Printing Dunn and Silhouette for Single linkage")
pDunnSil(hclust.single)
print("Printing Dunn and Silhouette for Complete linkage")
pDunnSil(hclust.complete)
print("Printing Dunn and Silhouette for Average linkage")
pDunnSil(hclust.average)
rm(hclust.single,hclust.complete,hclust.average)
```

##(g) According to the Dunn index, the best linkage is the Single Linkage. Using the Silhouette method the best linkage strategy is also Single linkage.

#2.3 Problem 3: K-Means and PCA
```{r}
set.seed(1122)
htru.df <-read.csv(file = "HTRU_2-small.csv")
htru.pca <- prcomp(scale(htru.df[,1:8]))
```


```{r}
htru.pca$sdev
summary(htru.df)
```

##(a) Perform PCA on the dataset and answer the following questions:
###(i) Variance explained by first two components
```{r}
cat(paste("The first two components explain:\n",
          format(100*sum(htru.pca$sdev[1:2])/sum(htru.pca$sdev),digits=4),
          "% of the variance"
          )
    )
```

###(ii) Plotting the first two principal components. Using a different color to represent the observations in the two classes.

```{r}
first2 <-htru.pca$x[,1:2]
  plot(first2[,1],
       first2[,2],
       col=rainbow(n=1,v = c( htru.df$class==1,htru.df$class==0)),
       xlab = paste("PC1 (",
                    format(100*sum(htru.pca$sdev[1])/sum(htru.pca$sdev),digits=4),
                    "%)"),
       ylab = paste("PC2 (",
                    format(100*sum(htru.pca$sdev[2])/sum(htru.pca$sdev),digits=4),
                  "%)"),
       main="Plotting using PCA with first two components"
      )
```


###(iii) By observing the plotted HTRU2 dataset using PCA we can see that most of the the class labels can be explained by PC1 being greater or smaller than about 2

##(b) We know that the HTRU2 dataset has two classes. We will now use K-means on the HTRU2 dataset.
###(i) Performing K-means clustering on the dataset with centers = 2, and nstart = 25 and plotting the resulting clusters.
```{r}
khtwu<- kmeans(scale(htru.df[,1:8]), centers=2,nstart=25)
fviz_cluster(khtwu, 
             data=scale(htru.df),
             main="Plotting using Kmeans Using all components")

```

###(ii) The shape of the clusters looks fairly similar since the majority of the variance that is explained in Kmeans using all attributes is also explained in PCA using the first attributes only (after axis rotation). This shows why dimensionality reduction is possible although there is an important lost of variance, the PCA clustering still have some predictive power.

###(iii) What is the distribution of the observations in each cluster?
```{r}
cat(paste0("In the Left  cluster (2), the percentage of observations is: ",
          format(100*sum(khtwu$cluster==2)/sum(khtwu$cluster==khtwu$cluster),digits=5)," %\n",
          "In the Right cluster (1), the percentage of observations is: ",
          format(100*sum(khtwu$cluster==1)/sum(khtwu$cluster==khtwu$cluster),digits=5)," %\n"))
```

###(iv) What is the distribution of the classes in the HTRU2 dataset?
```{r}
cat(paste0("The 'TRUE'  class label percentage of observations is:  ",
          format(100*sum(htru.df$class==1)/sum(htru.df$class==htru.df$class),digits=5),
          " %\n",
          "The 'FALSE' class label percentage of observations is: ",
          format(100*sum(htru.df$class==0)/sum(htru.df$class==htru.df$class),digits=5),
          " %\n"
          )
    )

```

###(v) Based on the distribution of the classes in (b)(iii) and (b)(iv), I believe that the cluster 1 correspond to the majority class while the cluster 2 correspond to the minority class.

###(vi) 
```{r}
cat(paste0("From the larger cluster, ",
           format(100*sum(htru.df[which(khtwu$cluster==1),9]==0)/sum(khtwu$cluster==1),
                  digits=4),
           " % belong to the majority class (0), while only ",
           format(100*sum(htru.df[which(khtwu$cluster==1),9]==1)/sum(khtwu$cluster==1),
                  digits=4),
           " % belong to the minority class (1)"
           )
    )
```

###(vii) I Still think that the larger cluster represents the majority class label (0). If we measure the model evaluation metrics we can see that the predictability is reasonable for this cluster.
```{r}
tn <- 100*sum(htru.df[which(khtwu$cluster==1),9]==0)
fn <- 100*sum(htru.df[which(khtwu$cluster==1),9]==1)
tp <- 100*sum(htru.df[which(khtwu$cluster==2),9]==1)
fp <- 100*sum(htru.df[which(khtwu$cluster==2),9]==0)
cat(paste0(
        "As a predictive model, it presents the following results:\n\taccuracy\t: ",
        format(100*(tn+tp)/(tp+tn+fp+fn),digits=4),
        "%\n\tError rate\t: ",
        format(100*(fn+fp)/(tp+tn+fp+fn),digits=4),
        "%\n\tPresicion\t: ",
        format(100*(tn)/(tn+fp),digits=4),
        "%\n\tSpecificity\t: ",
        format(100*(tp)/(tp+fp),digits=4),
        "%\n\t\tThis results are given that the positive class was the minority class (1)",
        "\nAlso, the balance accuracy (",
        format(100*((tp)/(tp+fp)+(tn)/(tn+fp))/2,digits=4)        ,
        "%) is pretty good for such an imbalanced sample: "
    )
)
rm(tn,fn,tp,fp)
```
```{r}
  #htru.dist <-dist(scale(htru.df[,1:8]))
  #clusStats<-fpc::cluster.stats(htru.dist,khtwu$cluster)
```

###(viii) 
```{r}

  cat(paste0("The total Variance explained by the clustering is : ",
            format(100*(khtwu$tot.withinss/khtwu$totss),digits=4),
            " %")
  )
```


###(ix)
```{r}
#cat(paste("Average Silhouette width of both of the cluster is:",format(clusStats$avg.silwidth,digits=4) ,"\n"))
silho<-silhouette(khtwu$cluster, dist(scale(htru.df[,1:8])))
cat(paste0("The average Silhouette width of both of the clusters is: ",
           format(sum(silho[,3])/sum(silho[,1]==silho[,1]),digits=4)
           )
    )
```

###(x)Based on the Silhouette width per cluster, cluster 1 is better
```{r}
for (x in 1:2){
  cat(paste0("The average Silhouette width of Cluster ",x," is: ",
           format(sum(silho[which(silho[,1]==x),3])/sum(silho[,1]==x),digits=4)
           ,"\n"
           )
    )
}
#print("Average Silhouette width per cluster")
#clusStats$clus.avg.silwidths

```
                                                                                         
##(c) Performing K-means on the result of the PCA in (a). Using only the first two principal component score vectors.

```{r}
set.seed(1122)
khtwu<- kmeans(scale(htru.pca$x[,1:2]), centers=2,nstart=25)
```

###(i) In the following plot it maintains the general shape but the clusters changed greatly, in this case cluster 2 grew considerably into cluster 1.
```{r}
fviz_cluster(khtwu, 
             data=scale(htru.df),
             main="Plotting using Kmeans Using PCA 2 Principal components")
```
###(ii) Average Silhouette width for both clusters?
```{r}
silho<-silhouette(khtwu$cluster, dist(scale(htru.pca$x[,1:8])))
cat(paste0("The average Silhouette width of both of the clusters is: ",
           format(sum(silho[,3])/sum(silho[,1]==silho[,1]),digits=4)
           )
    )
  
```

###(iii) The average Silhouette width is better for cluster 2, therefore, cluster 2 is the better cluster.
```{r}
for (x in 1:2){
  cat(paste0("The average Silhouette width of cluster ",x," is: ",
           format(sum(silho[which(silho[,1]==x),3])/sum(silho[,1]==x),digits=4)
           ,"\n"
           )
    )
}
```

###(iv) All Silhouette values in part C are smaller than the Silhouette values in part B. However, the general tendencies are roughly the same (The centroids are exchanged in this case too). This indicate that although dimensionality reduction takes a great deal of variance and quality of the clusters, it still provides the same information about general tendencies.

```{r}
rm(list=ls())
```

